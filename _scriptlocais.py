# -*- coding: utf-8 -*-
"""_scriptLocais.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wGGFTtkEEztgCnuQ28clZFdVh8FDQceL
"""

# Instala os pacotes necessários
!pip install python-igraph
!pip install networkx pandas

# Importa e renomeia os pacotes
import igraph as ig
import pandas as pd
import networkx as nx
import csv

from google.colab import drive
drive.mount('/content/drive')
# Define o caminho dos arquvos CSV
nos_csv = #Arquivo de nós
arestas_csv = #Arquivo de arestas

# Lê o arquivo CSV e cria um DataFrame com suas determinadas colunas
nos = pd.read_csv(nos_csv, header=None, names=["id"], dtype={"id":str})
num = len(nos)
print(num)
arestas = pd.read_csv(arestas_csv, header=None, names=["from", "to", "weight"], dtype={"from": str, "to": str, "weight": float}, sep="\t")

# Cria um grafo não direcionado
g = nx.Graph()

# # Adiciona os nós ao grafo
g.add_nodes_from(nos["id"])

#Itera pelo Dataframe de arestas e adiciona elas ao grafo
for _, row in arestas.iterrows():
    # Para cada linha do Dataframe, adiciona uma aresta entre os nós 'from' e 'to' com o peso 'weight'
    g.add_edge(str(row["from"]), str(row["to"]), weight=row["weight"])

import matplotlib.pyplot as plt

# Vendo como ficou o grafo
plt.figure(figsize=(10, 10))  # Define o tamanho da imagem
nx.draw(g, with_labels=True, node_color='pink', edge_color='gray', node_size=500, font_size=9)
plt.show()

"""Métricas Locais"""

#centralidade de intermediação/betweenness centrality)
betweenness = nx.betweenness_centrality(g, normalized=True, weight=None)

# centralidade de proximidade/closeness centrality
closeness = nx.closeness_centrality(g)

# grau dos vértices
degree = dict(g.degree())

# page rank
pagerank = nx.pagerank(g)

# centrality/centralidade de autovalor
centrality = nx.eigenvector_centrality(g)

# nome dos vértices
name = list(g.nodes)

# componentes conexos
comp_connected = list(nx.connected_components(g))

#cliques
cliques = list(nx.find_cliques(g))

# comprimento médio dos caminhos mínimos
ave_short_path = nx.average_shortest_path_length(g)

# cluster coefficient
cluster_coefficient = nx.clustering(g)

# Assortatividade de grau
assortativity_degree = nx.degree_assortativity_coefficient(g)

# diâmetro do grafo
if nx.is_connected(g):
    diameter = nx.diameter(g)
else:
    diameter = "Grafo Desconectado"

# Densidade
density = nx.density(g)

print(f"Componentes Conexos: {comp_connected}")
print(f"CLiques: {cliques}")
print(f"Comp. Médio dos caminhos mínimos: {ave_short_path}")
print(f"Assortividade de grau: {assortativity_degree}")
print(f"Diâmetro: {diameter}")
print(f"Densidade: {density}")

import random

func_csv = "/content/drive/MyDrive/Análise de redes sociais criminais/func.csv"
function = {}

with open(func_csv, newline='', encoding='utf-8') as csvfile:
    reader = csv.reader(csvfile, delimiter=',')

    for row in reader:
        if len(row) >= 2:
            chave = row[0].strip()
            valor = row[1].strip()
            function[chave] = valor

for item in nos.iloc[:, 0]:
    if item not in function:
        function[item] = "não sei"


print(function)

others = ["advogado", "coord_comunicacoes", "agente_de_lavagem"]

for node in function:
  if function[node] in others:
    function[node] = "liderança"

###################### Altera quantidade de vizinhos conhecidos ##################################

chaves = list(function.keys())
print(f"chaves: {chaves}")
function_alter = function.copy()
#n = len(chaves) # Não conhece nenhum vizinho
n = round(len(chaves)/2) # Conhece 50% dos vizinhos
#n = 0 # Conhece todos os vizinhos
chaves_escolhidas = random.sample(chaves, n)
print(f"chaves_escolhidas: {chaves_escolhidas}")

for chave in chaves_escolhidas:
  function_alter[chave] = "não sei"

with open("metricas_50_%.txt", "w") as file:
    all_functions = sorted(set(function_alter.values()))
    function_columns = "\t".join(all_functions)
    function_columns = function_columns
    print(function_columns)

    file.write(f"ID\tBetweenness\tCloseness\tDegree\tPage_Rank\tCentrality\tCluster_Coefficient\tFUNÇÃO\t{function_columns}\n")

    # Para cada nó no grafo, ele calcula as métricas
    for node in g.nodes():
        betw = betweenness.get(node, 0)
        close = closeness.get(node, 0)
        deg = degree.get(node, 0)
        rank = pagerank.get(node, 0)
        cent = centrality.get(node, 0)
        cc = cluster_coefficient.get(node, 0)
        func = function.get(node, "não sei")

        neighbors = g.neighbors(node)
        neighbor_function = [function_alter[n] for n in neighbors if n in function_alter]
        total_neighbors = len(neighbor_function)

        if total_neighbors > 0:
            function_counts = {
                c: (neighbor_function.count(c) / total_neighbors) * 100
                for c in all_functions
            }
        else:
            function_counts = {c: 0.0 for c in all_functions}

        function_values = "\t".join(f"{function_counts[c]:.2f}" for c in all_functions)
        file.write(f"{node}\t{betw}\t{close}\t{deg}\t{rank}\t{cent}\t{cc}\t{func}\t{function_values}\n")

df = pd.read_csv("metricas_100_%.txt", sep="\t")
df

